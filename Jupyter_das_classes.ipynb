{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento da Consulta\n",
    "\n",
    "A função query_process transforma a entrada inicial de uma pergunta em uma consulta. Palavras que não sejam nomes próprios (NPROP), números (NUM), verbos (V), substantivos (N) e adjetivos (ADJ) são removidos. Para a função é utilizado a biblioteca nlpnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpnet\n",
    "\n",
    "def query_process(texto_entrada):\n",
    "    #Diretorio dos modelos de etiquetação\n",
    "    data_dir = 'pos-pt'\n",
    "    #Definição do diretorio e linguagem a utilizar\n",
    "    tagger = nlpnet.POSTagger(data_dir, language='pt')\n",
    "    tagged_str = tagger.tag(texto_entrada)\n",
    "    #print(tagged_str)\n",
    "\n",
    "    texto_consulta = []\n",
    "    for i in tagged_str:\n",
    "        #print(i)\n",
    "        for k in i:\n",
    "            # print(k)\n",
    "            tags = ['NPROP', 'NUM', 'V', 'ADJ', 'N']\n",
    "            entrada = k\n",
    "            for i in tags:\n",
    "                # print(i)\n",
    "                if entrada[1] == i:\n",
    "                    texto_consulta.append(entrada[0])\n",
    "\n",
    "    return str(texto_consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperação de informação\n",
    "\n",
    "A função similaridade_sentencas recebe como entrada uma sentença(neste exemplo uma pergunta) é retorna uma passagem com maior valor de similaridade. Para esta função são utilizados TF-IDF e similaridade do cosseno.\n",
    "\n",
    "Atenção: Antes de utilizar esta função carregar base de documento para se buscar a resposta similia\n",
    "\n",
    "A função pre_processamento será mostrada em seguida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Atenção-------------------------------\n",
    "corpus_texto = open('/content/estatuto_uespi.txt','r',encoding='utf8')\n",
    "corpus = []\n",
    "for i in corpus_texto:\n",
    "    corpus.append(i)\n",
    "corpus_join = ''.join(corpus)\n",
    "corpus = corpus_join.split() \n",
    "\n",
    "lista_sentencas = nltk.sent_tokenize(corpus_replace)\n",
    "len(lista_sentencas)\n",
    "lista_sentencas_pre_processadas = []\n",
    "\n",
    "for frases in lista_sentencas:\n",
    "    lista_sentencas_pre_processadas.append(pre_processamento_texto(frases))\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# calcular similariedade do cosseno + TF-IDF\n",
    "def similaridade_sentencas(sentenca):\n",
    "    lista_similaridade = []\n",
    "    # recebe a sentença para se analisar é ja faz o pré-processamento\n",
    "    sentenca_entrada = pre_processamento_texto(sentenca)\n",
    "    # adicionar a sentenca de entrada no conjunto do regimento\n",
    "    lista_sentencas.append(sentenca)\n",
    "    lista_sentencas_pre_processadas.append(sentenca_entrada)\n",
    "\n",
    "    # convertendo base de dados para tf-idf\n",
    "    tfidf = TfidfVectorizer()\n",
    "    palavras_vetorizadas = tfidf.fit_transform(lista_sentencas_pre_processadas)\n",
    "\n",
    "    # calculo da similariedade\n",
    "    # pega a ultima frase que foi adicionada que no caso e nossa 'sentenca_entrada'\n",
    "    similaridade = cosine_similarity(palavras_vetorizadas[-1], palavras_vetorizadas)\n",
    "\n",
    "    # ordena por ordem decrescente cada indice\n",
    "    similaridade.argsort()\n",
    "\n",
    "    # 5 maiores resultados de similariedade\n",
    "    n1 = similaridade.argsort()[0][-2]\n",
    "\n",
    "    lista_similaridade.append(str(lista_sentencas[n1]))\n",
    "   \n",
    "    # remove a sentenca adicionada\n",
    "    del (lista_sentencas[-1])\n",
    "    del (lista_sentencas_pre_processadas[-1])\n",
    "\n",
    "    return str(lista_similaridade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de pre_processamento.\n",
    "\n",
    "Função de pre_processamento de texto, remove pontuações, stopwords, numeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementação das funcões para pré-processamento dos texto\n",
    "import importlib\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "importlib.reload(sys)\n",
    "\n",
    "#------------------------------------------------\n",
    "stopwords_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "#--------------------------------------------------\n",
    "#-------------------------------------------------\n",
    "#removendo pontuações\n",
    "#removendo stopwords com nltk\n",
    "#removendo numeros\n",
    "#-----------------------------------------------\n",
    "def pre_processamento_texto(texto):\n",
    "    #texto = str(texto)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    lista_de_palavras = []\n",
    "    palavras = texto.split()\n",
    "    for token in palavras:\n",
    "        novo_token = regex.sub(u'',token)\n",
    "        if not novo_token == u'':\n",
    "            lista_de_palavras.append(novo_token)\n",
    "    content = [w for w in lista_de_palavras if w.lower().strip() not in stopwords_nltk]\n",
    "\n",
    "    texto_limpo = []\n",
    "    for word in content:\n",
    "        nfkd = unicodedata.normalize('NFKD', word)\n",
    "        palavras_sem_acento = u''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "\n",
    "        q = re.sub('[^a-zA-Z0-9 \\\\\\]', ' ', palavras_sem_acento)\n",
    "\n",
    "        texto_limpo.append(q.lower().strip())\n",
    "\n",
    "    tokens = [t for t in texto_limpo if len(t)>2 and not t.isdigit()]\n",
    "    result = ' '.join(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
